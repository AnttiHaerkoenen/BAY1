{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference 2: Project Report\n",
    "\n",
    "### Title: \"\"\n",
    "\n",
    "### Name: Antti Härkönen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FOWuvoRrkHNb"
   },
   "source": [
    "## Abstract\n",
    "\n",
    "> A graphical Bayesian regression model with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1xVTgad0mdpj"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This project deals with the spatial segregation of ethnic groups in the city of Vyborg in 1880. At the time the city was\n",
    "a part of Autonomous Grand Duchy of Finland, which was a part of the Russian Empire. The city had unusually diverse, but\n",
    "mostly Lutheran, population. The members of the sizable Russian minority were mainly members Russian Orthodox church and\n",
    "had a distinct identity, but little is known about how well they were integrated with the other population groups. One\n",
    "important indicator of integration is spatial segregation. This can be studied using archival materials, but measuring\n",
    "different kinds segregation is difficult. Obviously, there is some level of economic segregation, where the richer merchants\n",
    "lived in the better parts of the town. Also, when the population of any minority group is affected by the same spatial\n",
    "conditions as everyone, which means that the general population density has to be accounted for when studying the spatial\n",
    "distributions of a minority group. My aim is to determine whether a hidden segregation variable can be inferred using\n",
    "the existing data. In a regression context, this would mean studying whether or not the coefficient for the effect of\n",
    "the hidden variable is around zero.\n",
    "\n",
    "A Generalized linear model with Poisson distribution is used to model the size of Orthodox population in a location.\n",
    "One of the variables used to predict number of Orthodox people is a hypothetical unobserved variable S, which stands for\n",
    "segregation which is not caused by income differences. Other unobserved variables are the regression coefficients for the\n",
    "Poisson regression.\n",
    "\n",
    "Python libraries used in this project are the following:\n",
    "* pymc3 for specifying Bayesian models\n",
    "* geopandas for reading, manipulating and plotting geospatial data\n",
    "* pandas and numpy for managing data\n",
    "* matplotlib for scatter plots and histograms\n",
    "* arviz for visualising results of Bayesian computations\n",
    "\n",
    "The aim of this project is to learn about the graphical models that could be used for studying challenging statistical\n",
    "inference problems.\n",
    "To my knowledge, there are no comparable studies of modeling high resolution spatial segregation that could be used as\n",
    "guidelines in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtBpp8W1pmEP"
   },
   "source": [
    "## Methods\n",
    "\n",
    "### Dataset description and processing\n",
    "\n",
    "A spatial dataset containing demographic and tax data from the city Vyborg in 1880 is used as data. The data is created\n",
    "by combining data from following sources:\n",
    "* Religious affiliation of inhabitants of each household is recorded in the poll tax register of 1880\n",
    "* Paid income tax is recorded in the Income tax register of 1880\n",
    "* Location of each city plot is determined using a variety of digitized, georeferenced and vectorized historical maps\n",
    "\n",
    "These sources are combined using plot numbers found in each source and joining them with Python tools such as pandas and\n",
    "geopandas. Since the data is 140 years old, modern regulations about storing personal information do not apply.\n",
    "\n",
    "There are three columns that are relevant for current model in the dataset:\n",
    "* total: Total number of inhabitants in location k\n",
    "* orthodox: Total number of Eastern Orthodox inhabitants in location k\n",
    "* income_per_person: Income per capita in location k\n",
    "\n",
    "The number of orthodox people is proxy for ethnically Russian population, since well over 90 percent of Orthodox civilian\n",
    "population of Vyborg were Russians.\n",
    "\n",
    "The population variables are discrete count data, while per capita income is a continuous variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '..\\\\data\\\\district_codes_1878.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-1-edbcf8c49c7e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     11\u001B[0m \u001B[0mwarnings\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msimplefilter\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0maction\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'ignore'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcategory\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mFutureWarning\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m \u001B[0mdistrict_codes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_dir\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;34m'district_codes_1878.csv'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     14\u001B[0m \u001B[0mdistrict_codes\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m{\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mv\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mv\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mdistrict_codes\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mitertuples\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[0minstitutions\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_dir\u001B[0m \u001B[1;33m/\u001B[0m \u001B[1;34m'institutions.csv'\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m{\u001B[0m\u001B[1;34m'district'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'plot_number'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m}\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\BAY2\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[0;32m    608\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    609\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 610\u001B[1;33m     \u001B[1;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    611\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    612\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\BAY2\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    461\u001B[0m     \u001B[1;31m# Create the parser.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 462\u001B[1;33m     \u001B[0mparser\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    463\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    464\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\BAY2\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m    817\u001B[0m             \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m\"has_index_names\"\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    818\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 819\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    820\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    821\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\BAY2\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[1;34m(self, engine)\u001B[0m\n\u001B[0;32m   1048\u001B[0m             )\n\u001B[0;32m   1049\u001B[0m         \u001B[1;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1050\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore[call-arg]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1051\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1052\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\BAY2\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, src, **kwds)\u001B[0m\n\u001B[0;32m   1865\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1866\u001B[0m         \u001B[1;31m# open handles\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1867\u001B[1;33m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1868\u001B[0m         \u001B[1;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1869\u001B[0m         \u001B[1;32mfor\u001B[0m \u001B[0mkey\u001B[0m \u001B[1;32min\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;34m\"storage_options\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"encoding\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"memory_map\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"compression\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\BAY2\\lib\\site-packages\\pandas\\io\\parsers.py\u001B[0m in \u001B[0;36m_open_handles\u001B[1;34m(self, src, kwds)\u001B[0m\n\u001B[0;32m   1360\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHanldes\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[1;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1361\u001B[0m         \"\"\"\n\u001B[1;32m-> 1362\u001B[1;33m         self.handles = get_handle(\n\u001B[0m\u001B[0;32m   1363\u001B[0m             \u001B[0msrc\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1364\u001B[0m             \u001B[1;34m\"r\"\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\Anaconda3\\envs\\BAY2\\lib\\site-packages\\pandas\\io\\common.py\u001B[0m in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    640\u001B[0m                 \u001B[0merrors\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"replace\"\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    641\u001B[0m             \u001B[1;31m# Encoding\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 642\u001B[1;33m             handle = open(\n\u001B[0m\u001B[0;32m    643\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    644\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '..\\\\data\\\\district_codes_1878.csv'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_dir = Path('../data')\n",
    "figure_dir = Path('../figures')\n",
    "figure_dir.mkdir(exist_ok=True)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "district_codes = pd.read_csv(data_dir / 'district_codes_1878.csv')\n",
    "district_codes = {k: v for k, v in district_codes.itertuples(index=False)}\n",
    "institutions = pd.read_csv(data_dir / 'institutions.csv', dtype={'district': str, 'plot_number': str})\n",
    "\n",
    "districts_in_city = [\n",
    "    'Valli',\n",
    "    'Salakkalahti',\n",
    "    'Repola',\n",
    "    'Anina',\n",
    "    'Papula',\n",
    "    'P_Annan_kruunu',\n",
    "    'Hiekka',\n",
    "    'Pantsarlahti',\n",
    "    'Viipurin_esikaupunki',\n",
    "    'Paulovski',\n",
    "    'Havi',\n",
    "    'Saunalahti',\n",
    "    'Pietarin_esikaupunki',\n",
    "]\n",
    "\n",
    "poll_tax = pd.read_csv(data_dir / 'poll_tax_register.csv', index_col=0)\n",
    "poll_tax['orthodox'] = poll_tax['orthodox'].interpolate(method='linear')\n",
    "poll_tax['in_russia_orthodox'] = poll_tax['in_russia_orthodox'].interpolate(method='linear', limit=2).fillna(0)\n",
    "poll_tax['total'] = (poll_tax.total_women + poll_tax.total_men).interpolate(method='linear')\n",
    "remove = poll_tax.in_russia_orthodox.copy()\n",
    "remove.loc['1906':] = 0\n",
    "poll_tax['orthodox'] = poll_tax.orthodox - remove\n",
    "health_council = pd.read_csv(data_dir / 'health_council.csv', index_col=0).pop_orthodox\n",
    "health_council = health_council.reindex(range(1880, 1921), fill_value=np.nan).interpolate(method='slinear')\n",
    "foreigners = pd.read_csv(data_dir / 'foreigners.csv', index_col=0).russia\n",
    "language = pd.read_csv(data_dir / 'language.csv', index_col=0).russian\n",
    "religion = pd.read_csv(data_dir / 'religion.csv', index_col=0).orthodox\n",
    "difference = health_council - poll_tax.orthodox\n",
    "data = pd.DataFrame([\n",
    "    difference,\n",
    "    poll_tax.orthodox,\n",
    "    poll_tax.in_russia_orthodox,\n",
    "    poll_tax.total,\n",
    "    health_council,\n",
    "    foreigners,\n",
    "    language,\n",
    "    religion,\n",
    "]).T\n",
    "data.columns = [\n",
    "    'difference',\n",
    "    'register_orthodox',\n",
    "    'orthodox_in_russia',\n",
    "    'register_total',\n",
    "    'health_council_orthodox',\n",
    "    'Russian_citizens',\n",
    "    'Russian_speakers',\n",
    "    'census_orthodox',\n",
    "]\n",
    "# data.interpolate(method='linear', inplace=True)\n",
    "data.plot(marker='+', legend=True, figsize=(10, 7), ylim=(0, 25_000))\n",
    "data.plot(marker='+', legend=True, figsize=(10, 7), ylim=(0, 4_000))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZvGatL4qQda"
   },
   "source": [
    "### Model description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import daft\n",
    "from matplotlib import rc\n",
    "\n",
    "rc(\"font\", family=\"serif\", size=16)\n",
    "scale = 1.25\n",
    "pgm = daft.PGM()\n",
    "\n",
    "pgm.add_node('health_orthodox', r'$o_{h}$', 1, 2, fixed=False, scale=scale, observed=True)\n",
    "pgm.add_node('register_orthodox', r'$o_{r}$', 3, 2, fixed=False, scale=scale, observed=True)\n",
    "pgm.add_node('russia_orthodox', r'$o_{ro}$', 2, 1, fixed=False, scale=scale, observed=True)\n",
    "pgm.add_node('beta_orthodox_health', r'$\\beta_h$', 1, 3, fixed=False, scale=scale, observed=False)\n",
    "pgm.add_node('beta_orthodox_register', r'$\\beta_r$', 3, 3, fixed=False, scale=scale, observed=False)\n",
    "pgm.add_node('theta', r'$\\theta$', 4, 1, fixed=True, scale=scale, observed=False)\n",
    "pgm.add_node('z', r'$z$', 3, 1, fixed=False, scale=scale, observed=False)\n",
    "pgm.add_node('sigma_register', r'$\\sigma_r$', 4, 2, fixed=False, scale=scale, observed=False)\n",
    "\n",
    "pgm.add_edge('health_orthodox', 'register_orthodox', directed=True)\n",
    "pgm.add_edge('russia_orthodox', 'register_orthodox', directed=True)\n",
    "pgm.add_edge('z', 'register_orthodox', directed=True)\n",
    "pgm.add_edge('beta_orthodox_health', 'health_orthodox', directed=True)\n",
    "pgm.add_edge('beta_orthodox_health', 'beta_orthodox_register', directed=True)\n",
    "pgm.add_edge('beta_orthodox_register', 'register_orthodox', directed=True)\n",
    "pgm.add_edge('sigma_register', 'register_orthodox', directed=True)\n",
    "pgm.add_edge('theta', 'z', directed=True)\n",
    "\n",
    "pgm.add_plate([0.5, 0.5, 3, 2], 'T')\n",
    "\n",
    "pgm.render()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Model:\n",
    "\n",
    "$ o_r = N(\\mu_{o_h}, \\sigma_{o_h}^2) $\n",
    "\n",
    "$ \\mu_{o_h} = \\beta_0 + \\beta_1 t - o_{ro} z $\n",
    "\n",
    "$ \\sigma_{o_h} \\sim Exp(10) $\n",
    "\n",
    "$ \\theta = \\begin{bmatrix} 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\end{bmatrix}' $\n",
    "\n",
    "$ z \\sim Bernoulli(\\theta) $\n",
    "\n",
    "$ \\beta_h \\sim N\\begin{pmatrix}\\begin{bmatrix} 0.2 \\\\ 0.005 \\end{bmatrix},\n",
    " \\begin{bmatrix} 0.01 && 0 \\\\ 0 && 0.001 \\end{bmatrix}\\end{pmatrix} $\n",
    "\n",
    "$ \\beta_r \\sim N\\begin{pmatrix}\\begin{bmatrix} 0.21 \\times 0.3 \\\\ 0.004 \\end{bmatrix},\n",
    " \\begin{bmatrix} 0.005 && 0 \\\\ 0 && 0.001 \\end{bmatrix}\\end{pmatrix} $\n",
    "\n",
    "Joint probability of the model:\n",
    "\n",
    "For the most variables the priors chosen are wide. This is because there is little prior knowledge about them given lack\n",
    "of earlier research."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7N9EXoyq5C-"
   },
   "source": [
    "### Inference Method\n",
    "\n",
    "Hamiltonian Monte Carlo simulation is used to infer latent variables. The model is built using Python statistical inference library\n",
    "[PyMC3](https://docs.pymc.io/) which implements No U-Turn Sampler (NUTS), which is a gradient-based Monte Carlo method. Since $\\bf z$\n",
    "is a binary variable, it has to be estimated using Gibbs sampling step.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_Az8m3ZsRR_"
   },
   "source": [
    "## Experiment & Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_pm = data.iloc[26:-3,]\n",
    "standard = 15_000\n",
    "data_pm = data_pm / standard\n",
    "start = 1906\n",
    "data_pm['t'] = data_pm.index - start\n",
    "\n",
    "with pm.Model() as model_health_council_orthodox:\n",
    "    n, _ = data_pm.shape\n",
    "    t = np.asarray(data_pm.t)\n",
    "    beta_health_0 = pm.Normal(\n",
    "        \"beta_health_0\",\n",
    "        mu=0.2,\n",
    "        sigma=0.01,\n",
    "    )\n",
    "    beta_health_1 = pm.Normal(\n",
    "        \"beta_health_1\",\n",
    "        mu=0.005,\n",
    "        sigma=0.001,\n",
    "    )\n",
    "    # mu_health = tt.add(beta_health_0, tt.mul(beta_health_1, data_pm.t))\n",
    "    mu_health = beta_health_0 + t * beta_health_1\n",
    "    # sigma_total = pm.Exponential('sigma_total', 100)\n",
    "    health_council = pm.Normal(\n",
    "        'health_council',\n",
    "        mu=mu_health,\n",
    "        sigma=0.01,\n",
    "        observed=np.asarray(data_pm.health_council_orthodox),\n",
    "    )\n",
    "\n",
    "    trace_health = pm.sample(1000, tune=2000, init=\"advi+adapt_diag\")\n",
    "    prior_health = pm.sample_prior_predictive()\n",
    "    posterior_health = pm.sample()\n",
    "    posterior_pred_health = pm.sample_posterior_predictive(posterior_health)\n",
    "\n",
    "az.plot_posterior(posterior_health)\n",
    "az.plot_trace(posterior_health)\n",
    "az.plot_forest(posterior_health, combined=True, hdi_prob=0.95)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mu_beta_orthodox_0 = 0.21 * 0.3\n",
    "mu_beta_orthodox_1 = 0.004\n",
    "\n",
    "with pm.Model() as model_orthodox:\n",
    "    beta_orthodox_0 = pm.Normal(\n",
    "        \"beta_orthodox_0\",\n",
    "        mu=mu_beta_orthodox_0,\n",
    "        sigma=0.005,\n",
    "    )\n",
    "    beta_orthodox_1 = pm.Normal(\n",
    "        \"beta_orthodox_1\",\n",
    "        mu=mu_beta_orthodox_1,\n",
    "        sigma=0.001,\n",
    "    )\n",
    "    theta = 0.5\n",
    "    z = pm.Bernoulli('z', p=theta, shape=n)\n",
    "    sigma_total = pm.Exponential('sigma_orthodox', 10)\n",
    "    missing = np.asarray(data_pm.orthodox_in_russia) * z\n",
    "    mu_total = beta_orthodox_0 + beta_orthodox_1 * t - missing\n",
    "    total = pm.Normal(\n",
    "        'register',\n",
    "        mu=mu_total,\n",
    "        sigma=sigma_total,\n",
    "        observed=data_pm.register_orthodox,\n",
    "    )\n",
    "\n",
    "    trace = pm.sample(1000, tune=2000, init=\"advi+adapt_diag\")\n",
    "    prior = pm.sample_prior_predictive()\n",
    "    posterior = pm.sample()\n",
    "    posterior_pred = pm.sample_posterior_predictive(posterior)\n",
    "\n",
    "az.plot_posterior(posterior)\n",
    "az.plot_trace(posterior)\n",
    "az.plot_forest(posterior, combined=True, hdi_prob=0.95)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OW1YvhHOsTTe",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Discussion and conclusion\n",
    "\n",
    "The results do not indicate that there is a significant amount of segregation that would affect the distribution\n",
    "of Russian Orthodox population apart from wealth and general population density.\n",
    "\n",
    "However, more research is needed to determine whether there are differences between different districts of the city,\n",
    "as these results could result from the ecological fallacy, i.e. the analysis results may depend on the spatial scale of\n",
    "regression. One approach to studying the districts would be a hierarchical regression model with pooling of data.\n",
    "In any case, the problem of spatial segregation involves interactions between many known and unknown\n",
    "variables, and these kinds of problems must be studied using Bayesian multilevel models.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "BAY1_ReportForm.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}